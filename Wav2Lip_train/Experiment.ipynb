{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型训练\n",
    "模型的训练主要分为两个部分：\n",
    "1. Lip-Sync Expert Discriminator的训练。这里提供官方的预训练模型 [weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP)\n",
    "2. Wav2Lip 模型的训练。\n",
    "\n",
    "### 3.1 预训练Lip-Sync Expert\n",
    "#### 1. 网络的搭建 \n",
    "上面我们已经介绍了SyncNet的基本网络结构，主要有一系列的(Conv+BatchNorm+Relu)组成，这里我们对其进行了一些改进，加入了残差结构。为了方便之后的使用，我们对(Conv+BatchNorm+Relu)以及残差模块进行了封装。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are models from Wav2lip project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        if self.residual:\n",
    "            out += x\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nonorm_Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonorm_Conv2d(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            )\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2dTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dTranspose(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SyncNet from Wav2Lip project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncNet_color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SyncNet_color, self).__init__()\n",
    "        \n",
    "        ################TODO###################\n",
    "        #根据上面提供的网络结构图，补全下面卷积网络的参数\n",
    "\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
    "        \n",
    "        #########################TODO#######################\n",
    "        # 正向传播\n",
    "        face_embedding = self.face_encoder(face_sequences)\n",
    "        audio_embedding = self.audio_encoder(audio_sequences)\n",
    "\n",
    "        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
    "        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
    "\n",
    "        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
    "        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "        return audio_embedding, face_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wav2lip file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "class Wav2Lip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n",
    "\n",
    "            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "            \n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.face_decoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n",
    "\n",
    "        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()) \n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences):\n",
    "        # audio_sequences = (B, T, 1, 80, 16)\n",
    "        B = audio_sequences.size(0)\n",
    "\n",
    "        input_dim_size = len(face_sequences.size())\n",
    "        if input_dim_size > 4:\n",
    "            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n",
    "            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "\n",
    "        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n",
    "\n",
    "        feats = []\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "            feats.append(x)\n",
    "\n",
    "        x = audio_embedding\n",
    "        for f in self.face_decoder_blocks:\n",
    "            x = f(x)\n",
    "            try:\n",
    "                x = torch.cat((x, feats[-1]), dim=1)\n",
    "            except Exception as e:\n",
    "                print(x.size())\n",
    "                print(feats[-1].size())\n",
    "                raise e\n",
    "            \n",
    "            feats.pop()\n",
    "\n",
    "        x = self.output_block(x)\n",
    "\n",
    "        if input_dim_size > 4:\n",
    "            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n",
    "            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n",
    "\n",
    "        else:\n",
    "            outputs = x\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "class Wav2Lip_disc_qual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip_disc_qual, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\n",
    "            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\n",
    "            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\n",
    "            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\n",
    "            \n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n",
    "        self.label_noise = .0\n",
    "\n",
    "    def get_lower_half(self, face_sequences):\n",
    "        return face_sequences[:, :, face_sequences.size(2)//2:]\n",
    "\n",
    "    def to_2d(self, face_sequences):\n",
    "        B = face_sequences.size(0)\n",
    "        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "        return face_sequences\n",
    "\n",
    "    def perceptual_forward(self, false_face_sequences):\n",
    "        false_face_sequences = self.to_2d(false_face_sequences)\n",
    "        false_face_sequences = self.get_lower_half(false_face_sequences)\n",
    "\n",
    "        false_feats = false_face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            false_feats = f(false_feats)\n",
    "\n",
    "        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \n",
    "                                        torch.ones((len(false_feats), 1)).cuda())\n",
    "\n",
    "        return false_pred_loss\n",
    "\n",
    "    def forward(self, face_sequences):\n",
    "        face_sequences = self.to_2d(face_sequences)\n",
    "        face_sequences = self.get_lower_half(face_sequences)\n",
    "\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "\n",
    "        return self.binary_pred(x).view(len(x), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from models import SyncNet_color as SyncNet\n",
    "import Wav2Lip.audio as audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "# from Wav2Lip.hparams import hparams, get_image_list\n",
    "from Wav2Lip.hparams import hparams\n",
    "# import Wav2Lip.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0 #起始的step\n",
    "global_epoch = 0 #起始的epoch\n",
    "device = \"mps\" if torch.backends.mps.is_built() else torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "use_cuda = torch.cuda.is_available()#训练的设备 cpu or gpu\n",
    "# print('use_cuda: {}'.format(use_cuda))\n",
    "syncnet_T = 5 ## 每次选取200ms的视频片段进行训练，视频的fps为25，所以200ms对应的帧数为：25*0.2=5帧\n",
    "syncnet_mel_step_size = 16 # 200ms对应的声音的mel-spectrogram特征的长度为16.\n",
    "data_root=\"../Data_Collection/lrs2_preprocessed/\" #数据集的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_list(data_root, split):\n",
    "\tfilelist = []\n",
    "\n",
    "\twith open('../Data_Collection/lrs2_preprocessed/filelists/{}.txt'.format(split)) as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif ' ' in line: line = line.split()[0]\n",
    "\t\t\tfilelist.append(os.path.join(data_root, line))\n",
    "\n",
    "\treturn filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        # num_frames = (T x hop_size * fps) / sample_rate\n",
    "        start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        return: x,mel,y\n",
    "        x: 五张嘴唇图片\n",
    "        mel: 对应的语音的mel spectrogram\n",
    "        t: 同步or不同步\n",
    "        \n",
    "        \"\"\"\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            # print(len(img_names))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "            \n",
    "            \n",
    "            #随机决定是产生负样本还是正样本\n",
    "            if random.choice([True, False]):\n",
    "                y = torch.ones(1).float()\n",
    "                chosen = img_name\n",
    "            else:\n",
    "                y = torch.zeros(1).float()\n",
    "                chosen = wrong_img_name\n",
    "\n",
    "            window_fnames = self.get_window(chosen)\n",
    "            if window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = []\n",
    "            all_read = True\n",
    "            for fname in window_fnames:\n",
    "                img = cv2.imread(fname)\n",
    "                if img is None:\n",
    "                    all_read = False\n",
    "                    break\n",
    "                try:\n",
    "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "                except Exception as e:\n",
    "                    all_read = False\n",
    "                    break\n",
    "\n",
    "                window.append(img)\n",
    "\n",
    "            if not all_read: continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            # H x W x 3 * T\n",
    "            x = np.concatenate(window, axis=2) / 255.\n",
    "            x = x.transpose(2, 0, 1)\n",
    "            x = x[:, x.shape[1]//2:]\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "\n",
    "            return x, mel, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need update audio.py line 100 for latest librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=Dataset(\"train\")\n",
    "x,mel,t=ds[0]\n",
    "print(x.shape)\n",
    "print(mel.shape)\n",
    "print(t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mel[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[:3,:,:].transpose(0,2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数的定义\n",
    "logloss = nn.BCELoss() # 交叉熵损失\n",
    "def cosine_loss(a, v, y):#余弦相似度损失\n",
    "    \"\"\"\n",
    "    a: audio_encoder的输出\n",
    "    v: video face_encoder的输出\n",
    "    y: 是否同步的真实值\n",
    "    \"\"\"\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "    \n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, mel, y) in prog_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #####TODO###########\n",
    "            ####################\n",
    "            #补全模型的训练\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            \n",
    "\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "\n",
    "            if global_step % hparams.syncnet_eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
    "\n",
    "            prog_bar.set_description('Epoch: {} Loss: {}'.format(global_epoch, running_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
    "    #在测试集上进行评估\n",
    "    eval_steps = 1400\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    losses = []\n",
    "    while 1:\n",
    "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        averaged_loss = sum(losses) / len(losses)\n",
    "        print(averaged_loss)\n",
    "\n",
    "        return\n",
    "\n",
    "latest_checkpoint_path = ''\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n",
    "    #保存训练的结果 checkpoint\n",
    "    global latest_checkpoint_path\n",
    "    \n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    latest_checkpoint_path = checkpoint_path\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "    #读取指定checkpoint的保存信息\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoints/expert_checkpoints/\" #指定存储 checkpoint的位置\n",
    "checkpoint_path = 'checkpoints/expert_checkpoints/checkpoint_step000080000.pth'\n",
    "# 指定加载checkpoint的路径，第一次训练时不需要，后续如果想从某个checkpoint恢复训练，可指定。\n",
    "\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "############TODO#########\n",
    "#####Train Dataloader and Test Dataloader \n",
    "#### 具体的bacthsize等参数，参考 hparams.py文件\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=8)\n",
    "\n",
    "# Model\n",
    "#####定义 SynNet模型，并加载到指定的device上\n",
    "model = SyncNet_color().to(device)\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "####定义优化器，使用adam，lr参考hparams.py文件\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=1e-5)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "      checkpoint_dir=checkpoint_dir,\n",
    "      checkpoint_interval=hparams.syncnet_checkpoint_interval,\n",
    "      nepochs=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
