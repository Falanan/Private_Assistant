{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2lip Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from models import SyncNet_color as SyncNet\n",
    "import Wav2Lip.audio as audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "# from Wav2Lip.hparams import hparams, get_image_list\n",
    "from Wav2Lip.hparams import hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_list(data_root, split):\n",
    "\tfilelist = []\n",
    "\n",
    "\twith open('../Data_Collection/lrs2_preprocessed/filelists/{}.txt'.format(split)) as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tif ' ' in line: line = line.split()[0]\n",
    "\t\t\tfilelist.append(os.path.join(data_root, line))\n",
    "\n",
    "\treturn filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        if self.residual:\n",
    "            out += x\n",
    "        return self.act(out)\n",
    "\n",
    "class nonorm_Conv2d(nn.Module): #不需要进行 norm的卷积\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            )\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)\n",
    "\n",
    "class Conv2dTranspose(nn.Module):# 逆卷积，上采样\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        ############TODO###########\n",
    "        ## 完成self.conv_block: 一个逆卷积和batchnorm组成的 Sequential结构\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Lip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n",
    "\n",
    "            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "            \n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.face_decoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n",
    "\n",
    "        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()) \n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences):\n",
    "        # audio_sequences = (B, T, 1, 80, 16)\n",
    "        B = audio_sequences.size(0)\n",
    "\n",
    "        input_dim_size = len(face_sequences.size())\n",
    "        if input_dim_size > 4:\n",
    "            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n",
    "            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "\n",
    "        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n",
    "\n",
    "        feats = []\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "            feats.append(x)\n",
    "\n",
    "        x = audio_embedding\n",
    "        for f in self.face_decoder_blocks:\n",
    "            x = f(x)\n",
    "            try:\n",
    "                x = torch.cat((x, feats[-1]), dim=1)\n",
    "            except Exception as e:\n",
    "                print(x.size())\n",
    "                print(feats[-1].size())\n",
    "                raise e\n",
    "            \n",
    "            feats.pop()\n",
    "\n",
    "        x = self.output_block(x)\n",
    "\n",
    "        if input_dim_size > 4:\n",
    "            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n",
    "            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n",
    "\n",
    "        else:\n",
    "            outputs = x\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Lip_disc_qual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip_disc_qual, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\n",
    "            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\n",
    "            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\n",
    "            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\n",
    "            \n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n",
    "        self.label_noise = .0\n",
    "\n",
    "    def get_lower_half(self, face_sequences):\n",
    "        return face_sequences[:, :, face_sequences.size(2)//2:]\n",
    "\n",
    "    def to_2d(self, face_sequences):\n",
    "        B = face_sequences.size(0)\n",
    "        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "        return face_sequences\n",
    "\n",
    "    def perceptual_forward(self, false_face_sequences):\n",
    "        false_face_sequences = self.to_2d(false_face_sequences)\n",
    "        false_face_sequences = self.get_lower_half(false_face_sequences)\n",
    "\n",
    "        false_feats = false_face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            false_feats = f(false_feats)\n",
    "\n",
    "        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \n",
    "                                        torch.ones((len(false_feats), 1)).cuda())\n",
    "\n",
    "        return false_pred_loss\n",
    "\n",
    "    def forward(self, face_sequences):\n",
    "        face_sequences = self.to_2d(face_sequences)\n",
    "        face_sequences = self.get_lower_half(face_sequences)\n",
    "\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "\n",
    "        return self.binary_pred(x).view(len(x), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def read_window(self, window_fnames):\n",
    "        if window_fnames is None: return None\n",
    "        window = []\n",
    "        for fname in window_fnames:\n",
    "            img = cv2.imread(fname)\n",
    "            if img is None:\n",
    "                return None\n",
    "            try:\n",
    "                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "            window.append(img)\n",
    "\n",
    "        return window\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        if type(start_frame) == int:\n",
    "            start_frame_num = start_frame\n",
    "        else:\n",
    "            start_frame_num = self.get_frame_id(start_frame) # 0-indexing ---> 1-indexing\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "        \n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "    def get_segmented_mels(self, spec, start_frame):\n",
    "        mels = []\n",
    "        assert syncnet_T == 5\n",
    "        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n",
    "        if start_frame_num - 2 < 0: return None\n",
    "        for i in range(start_frame_num, start_frame_num + syncnet_T):\n",
    "            m = self.crop_audio_window(spec, i - 2)\n",
    "            if m.shape[0] != syncnet_mel_step_size:\n",
    "                return None\n",
    "            mels.append(m.T)\n",
    "\n",
    "        mels = np.asarray(mels)\n",
    "\n",
    "        return mels\n",
    "\n",
    "    def prepare_window(self, window):\n",
    "        # 3 x T x H x W\n",
    "        x = np.asarray(window) / 255.\n",
    "        x = np.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1) #随机选择一个视频id\n",
    "            vidname = self.all_videos[idx]\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            \n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)#随机选择帧\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            window_fnames = self.get_window(img_name)\n",
    "            wrong_window_fnames = self.get_window(wrong_img_name)\n",
    "            if window_fnames is None or wrong_window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = self.read_window(window_fnames)\n",
    "            if window is None:\n",
    "                continue\n",
    "\n",
    "            wrong_window = self.read_window(wrong_window_fnames)\n",
    "            if wrong_window is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                #读取音频\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "                #提取完整mel-spectrogram\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            # 分割 mel-spectrogram\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "            \n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n",
    "            if indiv_mels is None: continue\n",
    "\n",
    "            window = self.prepare_window(window)\n",
    "            y = window.copy()\n",
    "            window[:, :, window.shape[2]//2:] = 0.\n",
    "\n",
    "            wrong_window = self.prepare_window(wrong_window)\n",
    "            x = np.concatenate([window, wrong_window], axis=0)\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n",
    "            y = torch.FloatTensor(y)\n",
    "            return x, indiv_mels, mel, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=\"../Data_Collection/lrs2_preprocessed/\" #数据集的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 96, 96])\n",
      "torch.Size([5, 1, 80, 16])\n",
      "torch.Size([1, 80, 16])\n",
      "torch.Size([3, 5, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "ds=Dataset(\"train\")\n",
    "x, indiv_mels, mel, y=ds[0]\n",
    "print(x.shape)\n",
    "print(indiv_mels.shape)\n",
    "print(mel.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyncNet_color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SyncNet_color, self).__init__()\n",
    "        \n",
    "        ################TODO###################\n",
    "        #根据上面提供的网络结构图，补全下面卷积网络的参数\n",
    "\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
    "        \n",
    "        #########################TODO#######################\n",
    "        # 正向传播\n",
    "        face_embedding = self.face_encoder(face_sequences)\n",
    "        audio_embedding = self.audio_encoder(audio_sequences)\n",
    "\n",
    "        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
    "        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
    "\n",
    "        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
    "        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "        return audio_embedding, face_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = nn.BCELoss()\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "syncnet = SyncNet_color().to(device) # 定义syncnet 模型\n",
    "for p in syncnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "    \n",
    "#####L1 loss    \n",
    "recon_loss = nn.L1Loss()\n",
    "def get_sync_loss(mel, g):\n",
    "    g = g[:, :, :, g.size(3)//2:]\n",
    "    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n",
    "    # B, 3 * T, H//2, W\n",
    "    a, v = syncnet(mel, g)\n",
    "    y = torch.ones(g.size(0), 1).float().to(device)\n",
    "    return cosine_loss(a, v, y)\n",
    "\n",
    "def train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "\n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            disc.train()\n",
    "            model.train()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            ### Train generator now. Remove ALL grads. \n",
    "            #训练生成器\n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            g = model(indiv_mels, x)#得到生成的结果\n",
    "\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)# 从预训练的expert 模型中获得唇音同步的损失\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)#判别器的感知损失\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)#l1 loss，重建损失\n",
    "            \n",
    "            #最终的损失函数\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### Remove all gradients before Training disc\n",
    "            # 训练判别器\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "            disc_real_loss.backward()\n",
    "\n",
    "            pred = disc(g.detach())\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "            disc_fake_loss.backward()\n",
    "\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            running_disc_real_loss += disc_real_loss.item()\n",
    "            running_disc_fake_loss += disc_fake_loss.item()\n",
    "\n",
    "            # Logs\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "\n",
    "            running_l1_loss += l1loss.item()\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                running_sync_loss += sync_loss.item()\n",
    "            else:\n",
    "                running_sync_loss += 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss += perceptual_loss.item()\n",
    "            else:\n",
    "                running_perceptual_loss += 0.\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n",
    "\n",
    "\n",
    "            if global_step % hparams.eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n",
    "\n",
    "                    if average_sync_loss < .75:\n",
    "                        hparams.set_hparam('syncnet_wt', 0.03)\n",
    "\n",
    "            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n",
    "                                                                                        running_sync_loss / (step + 1),\n",
    "                                                                                        running_perceptual_loss / (step + 1),\n",
    "                                                                                        running_disc_fake_loss / (step + 1),\n",
    "                                                                                        running_disc_real_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, disc):\n",
    "    eval_steps = 300\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n",
    "    while 1:\n",
    "        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n",
    "            model.eval()\n",
    "            disc.eval()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "            pred = disc(g)\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "\n",
    "            running_disc_real_loss.append(disc_real_loss.item())\n",
    "            running_disc_fake_loss.append(disc_fake_loss.item())\n",
    "\n",
    "            sync_loss = get_sync_loss(mel, g)\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            running_l1_loss.append(l1loss.item())\n",
    "            running_sync_loss.append(sync_loss.item())\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss.append(perceptual_loss.item())\n",
    "            else:\n",
    "                running_perceptual_loss.append(0.)\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n",
    "                                                            sum(running_sync_loss) / len(running_sync_loss),\n",
    "                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n",
    "                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n",
    "                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n",
    "        return sum(running_sync_loss) / len(running_sync_loss)\n",
    "\n",
    "latest_wav2lip_checkpoint = ''\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n",
    "    global latest_wav2lip_checkpoint\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n",
    "    if 'disc' not in checkpoint_path:\n",
    "        latest_wav2lip_checkpoint = checkpoint_path\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/wav2lip_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m#checkpoint 存储的位置\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Dataset and Dataloader setup\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m Dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m      8\u001b[0m     train_dataset, batch_size\u001b[38;5;241m=\u001b[39mhparams\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mhparams\u001b[38;5;241m.\u001b[39mnum_workers)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"checkpoints/wav2lip_checkpoints\"  #checkpoint 存储的位置\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=4)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    " # Model\n",
    "model = Wav2Lip().to(device)####### 生成器模型\n",
    "disc = Wav2Lip_disc_qual().to(device)####### 判别器模型\n",
    "\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=hparams.initial_learning_rate,\n",
    "                       betas=(0.5, 0.999))#####adam优化器，betas=[0.5,0.999]\n",
    "disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n",
    "                            lr=hparams.disc_initial_learning_rate,\n",
    "                            betas=(0.5, 0.999))#####adam优化器，betas=[0.5,0.999]\n",
    "\n",
    "#继续训练的生成器的checkpoint位置\n",
    "# checkpoint_path=\"\"\n",
    "# load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "#继续训练的判别器的checkpoint位置\n",
    "# disc_checkpoint_path=\"\"\n",
    "# load_checkpoint(disc_checkpoint_path, disc, disc_optimizer, \n",
    "#                             reset_optimizer=False, overwrite_global_states=False)\n",
    "\n",
    "# syncnet的checkpoint位置，我们将使用此模型计算生成的帧和语音的唇音同步损失\n",
    "# latest_checkpoint_path = \"checkpoints/expert_checkpoints/\"\n",
    "latest_checkpoint_path = \"checkpoints/expert_checkpoints/checkpoint_step000080000.pth\"\n",
    "syncnet_checkpoint_path = latest_checkpoint_path\n",
    "# syncnet_checkpoint_path=\"/kaggle/working/expert_checkpoints/checkpoint_step000000001.pth\"\n",
    "load_checkpoint(syncnet_checkpoint_path, syncnet, None, reset_optimizer=True,\n",
    "                            overwrite_global_states=False)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "# Train!\n",
    "train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=checkpoint_dir,\n",
    "          checkpoint_interval=hparams.checkpoint_interval,\n",
    "          nepochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
